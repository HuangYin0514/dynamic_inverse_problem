{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbu/project/PINN_DE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "grandparent_dir = os.path.abspath(os.path.join(parent_dir, '..'))\n",
    "sys.path.append(grandparent_dir)\n",
    "print(grandparent_dir)\n",
    "\n",
    "import gendata\n",
    "import learner as ln\n",
    "from utils import Logger, read_config_file, set_random_seed, tensors_to_numpy\n",
    "\n",
    "from learner.metric.dynamics_metric import (\n",
    "    calculate_dynamics_metrics,\n",
    "    plot_dynamics_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 19:54:43 INFO ####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "from configs.config_plot import *\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)  \n",
    "\n",
    "seed = 0\n",
    "set_random_seed(seed)\n",
    "\n",
    "# Logger\n",
    "logger = Logger(output_dir)\n",
    "\n",
    "config_file_path = \"/home/lbu/project/PINN_DE/configs/test_something/config_model_test.py\"\n",
    "config = read_config_file(config_file_path)\n",
    "\n",
    "logger.info(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "# data setting\n",
    "#\n",
    "########################################################\n",
    "\n",
    "physics_t = torch.linspace(0, 3, 20, device=config.device, dtype=config.dtype).view(-1, 1)\n",
    "data_t = torch.linspace(0, 3, 10, device=config.device, dtype=config.dtype).view(-1, 1)\n",
    "\n",
    "y0 = torch.tensor([0., 1.], device=config.device, dtype=config.dtype)\n",
    "y = torch.cat([torch.sin(data_t), torch.cos(data_t)], dim=-1)\n",
    "yt = torch.cat([torch.cos(data_t), -torch.sin(data_t)], dim=-1)\n",
    "\n",
    "physics_t = physics_t.requires_grad_(True)\n",
    "data_t = data_t.requires_grad_(True)\n",
    "\n",
    "data = y0, y, yt, data_t, physics_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class SinActivation(nn.Module):\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return torch.sin(input)\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    MLPBlock represents a single block of the MLP (Multi-Layer Perceptron) model.\n",
    "\n",
    "    It consists of a linear layer followed by an activation function.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The number of input features to the block.\n",
    "        output_dim (int): The number of output features from the block.\n",
    "        activation (torch.nn.Module): The activation function to be applied after the linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_layer = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLPBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        return self.activation(self.linear_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PINN(\n",
       "  (backboneNet): BackboneNet(\n",
       "    (net): Sequential(\n",
       "      (0): MLPBlock(\n",
       "        (linear_layer): Linear(in_features=3, out_features=20, bias=True)\n",
       "        (activation): SinActivation()\n",
       "      )\n",
       "      (1): MLPBlock(\n",
       "        (linear_layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (activation): SinActivation()\n",
       "      )\n",
       "      (2): MLPBlock(\n",
       "        (linear_layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (activation): SinActivation()\n",
       "      )\n",
       "      (3): MLPBlock(\n",
       "        (linear_layer): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (activation): SinActivation()\n",
       "      )\n",
       "      (4): Linear(in_features=20, out_features=2, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (right_term_net): DynamicDoublePendulumDAE(\n",
       "    (phi_net): Phi_Net()\n",
       "    (m_net): M_Net()\n",
       "    (f_net): F_Net()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from learner.metric.dynamics_metric import (\n",
    "    calculate_dynamics_metrics,\n",
    "    plot_dynamics_metrics,\n",
    ")\n",
    "from utils import batched_jacobian, initialize_class, tensors_to_numpy\n",
    "\n",
    "\n",
    "class BackboneNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom backbone neural network module.\n",
    "\n",
    "    This module contains multiple MLPBlocks with a specified number of layers.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input features.\n",
    "        hidden_dim (int): Dimension of the hidden layers.\n",
    "        output_dim (int): Dimension of the output.\n",
    "        layers_num (int): Number of hidden layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BackboneNet, self).__init__()\n",
    "\n",
    "        input_dim = config.BackboneNet_input_dim\n",
    "        hidden_dim = config.BackboneNet_hidden_dim\n",
    "        output_dim = config.BackboneNet_output_dim\n",
    "        layers_num = config.BackboneNet_layers_num\n",
    "\n",
    "        activation = SinActivation()\n",
    "\n",
    "        # Input layer\n",
    "        input_layer = MLPBlock(input_dim, hidden_dim, activation)\n",
    "\n",
    "        # Hidden layers\n",
    "        hidden_layers = nn.ModuleList(\n",
    "            [MLPBlock(hidden_dim, hidden_dim, activation) for _ in range(layers_num)]\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(hidden_dim, output_dim, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        layers.extend([input_layer])\n",
    "        layers.extend(hidden_layers)\n",
    "        layers.extend([output_layer])\n",
    "\n",
    "        # Create the sequential model\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the BackboneNet.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, config, logger, *args, **kwargs):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "        self.device = config.device\n",
    "        self.dtype = config.dtype\n",
    "\n",
    "        self.backboneNet = BackboneNet(config)\n",
    "\n",
    "        try:\n",
    "            class_name = self.config.dynamic_class\n",
    "            kwargs = {\"config\": config, \"logger\": logger}\n",
    "            self.right_term_net = initialize_class(\"dynamics\", class_name, **kwargs)\n",
    "        except ValueError as e:\n",
    "            raise RuntimeError(\"class '{}' is not available\".format(class_name))\n",
    "\n",
    "    def forward(self, t, q0):\n",
    "        bs, _ = t.shape\n",
    "        t = t.reshape(-1, 1)\n",
    "        out = torch.zeros(bs, 2, device=self.device, dtype=self.dtype)\n",
    "        qi =  q0\n",
    "        for i, ti in enumerate(t):\n",
    "            input = torch.cat([ti,qi], dim=-1)\n",
    "            qi = self.backboneNet(input)\n",
    "            out[i] = qi\n",
    "        return out\n",
    "\n",
    "    def get_q_qt_qtt(self, t, q0):\n",
    "        q_hat = self(t, q0)\n",
    "        qt_hat = batched_jacobian(\n",
    "            q_hat, t, device=self.device, dtype=self.dtype\n",
    "        ).squeeze(-1)\n",
    "        qtt_hat = batched_jacobian(\n",
    "            qt_hat, t, device=self.device, dtype=self.dtype\n",
    "        ).squeeze(-1)\n",
    "        return q_hat, qt_hat, qtt_hat\n",
    "\n",
    "\n",
    "config.BackboneNet_input_dim = 3\n",
    "config.BackboneNet_hidden_dim = 20\n",
    "config.BackboneNet_output_dim = 2\n",
    "config.BackboneNet_layers_num = 3\n",
    "net_dp = PINN(config, logger).to(config.device).to(config.dtype)\n",
    "net_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0141, -0.0017],\n",
       "        [-0.0056,  0.0022],\n",
       "        [-0.0037,  0.0050],\n",
       "        [-0.0025,  0.0066],\n",
       "        [-0.0017,  0.0074],\n",
       "        [-0.0013,  0.0073],\n",
       "        [-0.0013,  0.0067],\n",
       "        [-0.0015,  0.0059],\n",
       "        [-0.0017,  0.0050],\n",
       "        [-0.0022,  0.0039]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_hat = net_dp(data_t, y0)\n",
    "q_hat.shape\n",
    "\n",
    "q_hat, qt_hat, qtt_hat = net_dp.get_q_qt_qtt(data_t, y0)\n",
    "qtt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
